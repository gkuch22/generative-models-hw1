{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z1yQ_hQg4Swr"
      },
      "outputs": [],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WIdCei7q5ErG"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BtpcfaYO5HEr"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "27hObL-YGPnl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0SqsP304ede"
      },
      "outputs": [],
      "source": [
        "# ==================== CONFIGURATION ====================\n",
        "class Config:\n",
        "    DATASET_PATH = '/content/drive/MyDrive/cs236/assignments/assignment1/datasets/'\n",
        "    CHECKPOINT_DIR = '/content/drive/MyDrive/cs236/assignments/assignment1/checkpoints/'\n",
        "\n",
        "    IMG_SIZE = 256\n",
        "    BATCH_SIZE = 4\n",
        "    NUM_EPOCHS = 30\n",
        "    SAVE_EVERY = 1\n",
        "\n",
        "    LR_G = 0.0002\n",
        "    LR_D = 0.0002\n",
        "    BETA1 = 0.5\n",
        "    BETA2 = 0.999\n",
        "\n",
        "    LOSS_TYPE = 'hinge'\n",
        "\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ==================== DATASET ====================\n",
        "class MonetPhotoDataset(Dataset):\n",
        "    def __init__(self, monet_dir, photo_dir, transform=None):\n",
        "        self.monet_paths = sorted(list(Path(monet_dir).glob('*.jpg')))\n",
        "        self.photo_paths = sorted(list(Path(photo_dir).glob('*.jpg')))\n",
        "        self.transform = transform\n",
        "\n",
        "        print(f\"Found {len(self.monet_paths)} Monet images\")\n",
        "        print(f\"Found {len(self.photo_paths)} Photo images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.monet_paths), len(self.photo_paths))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        monet_img = Image.open(self.monet_paths[idx % len(self.monet_paths)]).convert('RGB')\n",
        "        photo_img = Image.open(self.photo_paths[idx % len(self.photo_paths)]).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            monet_img = self.transform(monet_img)\n",
        "            photo_img = self.transform(photo_img)\n",
        "\n",
        "        return monet_img, photo_img\n",
        "\n",
        "\n",
        "# ==================== GENERATOR ====================\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(channels, channels, 3),\n",
        "            nn.InstanceNorm2d(channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(channels, channels, 3),\n",
        "            nn.InstanceNorm2d(channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, features=64, n_residual=9):\n",
        "        super().__init__()\n",
        "\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_channels, features, 7),\n",
        "            nn.InstanceNorm2d(features),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        curr_dim = features\n",
        "        for _ in range(2):\n",
        "            self.down_blocks.append(nn.Sequential(\n",
        "                nn.Conv2d(curr_dim, curr_dim * 2, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(curr_dim * 2),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ))\n",
        "            curr_dim *= 2\n",
        "\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            *[ResidualBlock(curr_dim) for _ in range(n_residual)]\n",
        "        )\n",
        "\n",
        "        self.up_blocks = nn.ModuleList()\n",
        "        for _ in range(2):\n",
        "            self.up_blocks.append(nn.Sequential(\n",
        "                nn.ConvTranspose2d(curr_dim, curr_dim // 2, 3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(curr_dim // 2),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ))\n",
        "            curr_dim //= 2\n",
        "\n",
        "        self.output = nn.Sequential(\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(curr_dim, out_channels, 7),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial(x)\n",
        "        for down in self.down_blocks:\n",
        "            x = down(x)\n",
        "        x = self.res_blocks(x)\n",
        "        for up in self.up_blocks:\n",
        "            x = up(x)\n",
        "        return self.output(x)\n",
        "\n",
        "\n",
        "# ==================== DISCRIMINATOR ====================\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=64):\n",
        "        super().__init__()\n",
        "\n",
        "        def discriminator_block(in_f, out_f, normalize=True):\n",
        "            layers = [nn.Conv2d(in_f, out_f, 4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_f))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(in_channels, features, normalize=False),\n",
        "            *discriminator_block(features, features * 2),\n",
        "            *discriminator_block(features * 2, features * 4),\n",
        "            *discriminator_block(features * 4, features * 8),\n",
        "            nn.Conv2d(features * 8, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# ==================== LOSSES ====================\n",
        "class LSGANLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def discriminator_loss(self, real_pred, fake_pred):\n",
        "        real_loss = self.mse(real_pred, torch.ones_like(real_pred))\n",
        "        fake_loss = self.mse(fake_pred, torch.zeros_like(fake_pred))\n",
        "        return (real_loss + fake_loss) * 0.5\n",
        "\n",
        "    def generator_loss(self, fake_pred):\n",
        "        return self.mse(fake_pred, torch.ones_like(fake_pred))\n",
        "\n",
        "class HingeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def discriminator_loss(self, real_pred, fake_pred):\n",
        "        real_loss = torch.mean(torch.relu(1.0 - real_pred))\n",
        "        fake_loss = torch.mean(torch.relu(1.0 + fake_pred))\n",
        "        return (real_loss + fake_loss) * 0.5\n",
        "\n",
        "    def generator_loss(self, fake_pred):\n",
        "        return -torch.mean(fake_pred)\n",
        "\n",
        "\n",
        "def get_adversarial_loss(loss_type):\n",
        "    if loss_type == 'lsgan':\n",
        "        return LSGANLoss()\n",
        "    elif loss_type == 'hinge':\n",
        "        return HingeLoss()\n",
        "\n",
        "def cycle_loss(real_img, reconstructed_img):\n",
        "    return nn.L1Loss()(real_img, reconstructed_img)\n",
        "\n",
        "def identity_loss(real_img, same_img):\n",
        "    return nn.L1Loss()(real_img, same_img)\n",
        "\n",
        "\n",
        "# ==================== TRAINING FUNCTIONS ====================\n",
        "def save_checkpoint(epoch, G_M2P, G_P2M, D_M, D_P, opt_G, opt_D, loss_type, checkpoint_dir):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'G_M2P_state': G_M2P.state_dict(),\n",
        "        'G_P2M_state': G_P2M.state_dict(),\n",
        "        'D_M_state': D_M.state_dict(),\n",
        "        'D_P_state': D_P.state_dict(),\n",
        "        'opt_G_state': opt_G.state_dict(),\n",
        "        'opt_D_state': opt_D.state_dict(),\n",
        "        'loss_type': loss_type\n",
        "    }\n",
        "\n",
        "    final_path = os.path.join(checkpoint_dir, f'{loss_type}_epoch_{epoch}.pth')\n",
        "    temp_path = final_path + '.tmp'\n",
        "\n",
        "    try:\n",
        "        torch.save(checkpoint, temp_path)\n",
        "        shutil.move(temp_path, final_path)\n",
        "        print(f\"Checkpoint saved: {final_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save checkpoint: {e}\")\n",
        "        if os.path.exists(temp_path):\n",
        "            os.remove(temp_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, G_M2P, G_P2M, D_M, D_P, opt_G, opt_D):\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        G_M2P.load_state_dict(checkpoint['G_M2P_state'])\n",
        "        G_P2M.load_state_dict(checkpoint['G_P2M_state'])\n",
        "        D_M.load_state_dict(checkpoint['D_M_state'])\n",
        "        D_P.load_state_dict(checkpoint['D_P_state'])\n",
        "        opt_G.load_state_dict(checkpoint['opt_G_state'])\n",
        "        opt_D.load_state_dict(checkpoint['opt_D_state'])\n",
        "        return checkpoint['epoch']\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to load checkpoint: {e}\")\n",
        "\n",
        "\n",
        "def find_valid_checkpoint(checkpoint_dir, loss_type):\n",
        "    checkpoints = []\n",
        "\n",
        "    for ckpt_path in Path(checkpoint_dir).glob(f'{loss_type}_epoch_*.pth'):\n",
        "        match = re.search(r'epoch_(\\d+)\\.pth', ckpt_path.name)\n",
        "        if match:\n",
        "            epoch = int(match.group(1))\n",
        "            checkpoints.append((epoch, ckpt_path))\n",
        "\n",
        "    checkpoints.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    for epoch, ckpt_path in checkpoints:\n",
        "        try:\n",
        "            checkpoint = torch.load(str(ckpt_path), map_location='cpu')\n",
        "            if 'epoch' in checkpoint and 'G_M2P_state' in checkpoint:\n",
        "                return str(ckpt_path)\n",
        "        except:\n",
        "            print(f\"Skipping corrupted checkpoint: {ckpt_path}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def visualize_results(real_monet, fake_photo, real_photo, fake_monet, epoch):\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "    def denorm(x):\n",
        "        return (x * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "    images = [\n",
        "        (real_monet[0], \"Real Monet\"),\n",
        "        (fake_photo[0], \"Fake Photo (M→P)\"),\n",
        "        (real_photo[0], \"Real Photo\"),\n",
        "        (fake_monet[0], \"Fake Monet (P→M)\"),\n",
        "    ]\n",
        "\n",
        "    for idx, (img, title) in enumerate(images):\n",
        "        img_np = denorm(img).cpu().permute(1, 2, 0).numpy()\n",
        "        axes[0, idx].imshow(img_np)\n",
        "        axes[0, idx].set_title(title)\n",
        "        axes[0, idx].axis('off')\n",
        "\n",
        "    if len(real_monet) > 1:\n",
        "        for idx in range(min(4, len(real_monet))):\n",
        "            if idx == 0:\n",
        "                img_np = denorm(fake_photo[idx]).cpu().permute(1, 2, 0).numpy()\n",
        "            elif idx == 1:\n",
        "                img_np = denorm(fake_monet[idx]).cpu().permute(1, 2, 0).numpy()\n",
        "            elif idx == 2:\n",
        "                img_np = denorm(real_monet[idx]).cpu().permute(1, 2, 0).numpy()\n",
        "            else:\n",
        "                img_np = denorm(real_photo[idx]).cpu().permute(1, 2, 0).numpy()\n",
        "            axes[1, idx].imshow(img_np)\n",
        "            axes[1, idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ==================== MAIN TRAINING ====================\n",
        "def train():\n",
        "    wandb.init(\n",
        "        project=\"cs236-gan-assignment\",\n",
        "        name=f\"cyclegan-{Config.LOSS_TYPE}\",\n",
        "        config={\n",
        "            'loss_type': Config.LOSS_TYPE,\n",
        "            'img_size': Config.IMG_SIZE,\n",
        "            'batch_size': Config.BATCH_SIZE,\n",
        "            'num_epochs': Config.NUM_EPOCHS,\n",
        "            'lr_g': Config.LR_G,\n",
        "            'lr_d': Config.LR_D,\n",
        "            'beta1': Config.BETA1,\n",
        "            'beta2': Config.BETA2\n",
        "        }\n",
        "    )\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    dataset = MonetPhotoDataset(\n",
        "        monet_dir=os.path.join(Config.DATASET_PATH, 'monet_jpg'),\n",
        "        photo_dir=os.path.join(Config.DATASET_PATH, 'photo_jpg'),\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "    G_M2P = Generator().to(Config.DEVICE)\n",
        "    G_P2M = Generator().to(Config.DEVICE)\n",
        "    D_M = Discriminator().to(Config.DEVICE)\n",
        "    D_P = Discriminator().to(Config.DEVICE)\n",
        "\n",
        "    opt_G = optim.Adam(\n",
        "        list(G_M2P.parameters()) + list(G_P2M.parameters()),\n",
        "        lr=Config.LR_G, betas=(Config.BETA1, Config.BETA2)\n",
        "    )\n",
        "    opt_D = optim.Adam(\n",
        "        list(D_M.parameters()) + list(D_P.parameters()),\n",
        "        lr=Config.LR_D, betas=(Config.BETA1, Config.BETA2)\n",
        "    )\n",
        "\n",
        "    adv_loss = get_adversarial_loss(Config.LOSS_TYPE)\n",
        "\n",
        "    start_epoch = 0\n",
        "    valid_checkpoint = find_valid_checkpoint(Config.CHECKPOINT_DIR, Config.LOSS_TYPE)\n",
        "\n",
        "    if valid_checkpoint:\n",
        "        print(f\"Found valid checkpoint: {valid_checkpoint}\")\n",
        "        try:\n",
        "            start_epoch = load_checkpoint(valid_checkpoint, G_M2P, G_P2M, D_M, D_P, opt_G, opt_D) + 1\n",
        "            print(f\"Resuming from epoch {start_epoch}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load checkpoint: {e}\")\n",
        "            print(\"Starting from scratch...\")\n",
        "            start_epoch = 0\n",
        "    else:\n",
        "        print(\"No valid checkpoint found. Starting from scratch...\")\n",
        "\n",
        "\n",
        "    for epoch in range(start_epoch, Config.NUM_EPOCHS):\n",
        "        G_M2P.train()\n",
        "        G_P2M.train()\n",
        "        D_M.train()\n",
        "        D_P.train()\n",
        "\n",
        "        epoch_losses = {'D_loss': 0, 'G_loss': 0, 'cycle_loss': 0, 'identity_loss': 0, 'adv_loss': 0}\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{Config.NUM_EPOCHS}\")\n",
        "        for batch_idx, (real_monet, real_photo) in enumerate(pbar):\n",
        "            real_monet = real_monet.to(Config.DEVICE)\n",
        "            real_photo = real_photo.to(Config.DEVICE)\n",
        "\n",
        "            # ==================== Train Discriminators ====================\n",
        "            opt_D.zero_grad()\n",
        "\n",
        "            fake_monet = G_P2M(real_photo)\n",
        "            pred_real_monet = D_M(real_monet)\n",
        "            pred_fake_monet = D_M(fake_monet.detach())\n",
        "            D_M_loss = adv_loss.discriminator_loss(pred_real_monet, pred_fake_monet)\n",
        "\n",
        "            fake_photo = G_M2P(real_monet)\n",
        "            pred_real_photo = D_P(real_photo)\n",
        "            pred_fake_photo = D_P(fake_photo.detach())\n",
        "            D_P_loss = adv_loss.discriminator_loss(pred_real_photo, pred_fake_photo)\n",
        "\n",
        "            D_loss = D_M_loss + D_P_loss\n",
        "            D_loss.backward()\n",
        "            opt_D.step()\n",
        "\n",
        "            # ==================== Train Generators ====================\n",
        "            opt_G.zero_grad()\n",
        "\n",
        "            pred_fake_monet = D_M(fake_monet)\n",
        "            pred_fake_photo = D_P(fake_photo)\n",
        "            G_M2P_adv_loss = adv_loss.generator_loss(pred_fake_photo)\n",
        "            G_P2M_adv_loss = adv_loss.generator_loss(pred_fake_monet)\n",
        "            total_adv_loss = G_M2P_adv_loss + G_P2M_adv_loss\n",
        "\n",
        "            reconstructed_monet = G_P2M(fake_photo)\n",
        "            reconstructed_photo = G_M2P(fake_monet)\n",
        "            cycle_M = cycle_loss(real_monet, reconstructed_monet)\n",
        "            cycle_P = cycle_loss(real_photo, reconstructed_photo)\n",
        "            total_cycle_loss = (cycle_M + cycle_P) * 10.0\n",
        "\n",
        "            identity_monet = G_P2M(real_monet)\n",
        "            identity_photo = G_M2P(real_photo)\n",
        "            identity_M = identity_loss(real_monet, identity_monet)\n",
        "            identity_P = identity_loss(real_photo, identity_photo)\n",
        "            total_identity_loss = (identity_M + identity_P) * 5.0\n",
        "\n",
        "            G_loss = G_M2P_adv_loss + G_P2M_adv_loss + total_cycle_loss + total_identity_loss\n",
        "            G_loss.backward()\n",
        "            opt_G.step()\n",
        "\n",
        "            epoch_losses['D_loss'] += D_loss.item()\n",
        "            epoch_losses['G_loss'] += G_loss.item()\n",
        "            epoch_losses['cycle_loss'] += total_cycle_loss.item()\n",
        "            epoch_losses['identity_loss'] += total_identity_loss.item()\n",
        "            epoch_losses['adv_loss'] += total_adv_loss.item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'D': f\"{D_loss.item():.4f}\",\n",
        "                'G': f\"{G_loss.item():.4f}\",\n",
        "                'Cyc': f\"{total_cycle_loss.item():.4f}\"\n",
        "            })\n",
        "\n",
        "        for key in epoch_losses:\n",
        "            epoch_losses[key] /= len(dataloader)\n",
        "\n",
        "        wandb.log({\n",
        "            'epoch': epoch + 1,\n",
        "            **epoch_losses\n",
        "        })\n",
        "\n",
        "        with torch.no_grad():\n",
        "            G_M2P.eval()\n",
        "            G_P2M.eval()\n",
        "            fake_photo = G_M2P(real_monet)\n",
        "            fake_monet = G_P2M(real_photo)\n",
        "            fig = visualize_results(real_monet, fake_photo, real_photo, fake_monet, epoch)\n",
        "            wandb.log({\"generated_images\": wandb.Image(fig)})\n",
        "            plt.close(fig)\n",
        "\n",
        "        if (epoch + 1) % Config.SAVE_EVERY == 0:\n",
        "            save_checkpoint(epoch, G_M2P, G_P2M, D_M, D_P, opt_G, opt_D, Config.LOSS_TYPE, Config.CHECKPOINT_DIR)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: D_loss={epoch_losses['D_loss']:.4f}, \"\n",
        "              f\"G_loss={epoch_losses['G_loss']:.4f}, \"\n",
        "              f\"Cycle={epoch_losses['cycle_loss']:.4f}\")\n",
        "\n",
        "    save_checkpoint(Config.NUM_EPOCHS - 1, G_M2P, G_P2M, D_M, D_P, opt_G, opt_D, Config.LOSS_TYPE, Config.CHECKPOINT_DIR)\n",
        "    wandb.finish()\n",
        "    print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PN6VfQj-8gNo"
      },
      "outputs": [],
      "source": [
        "# ==================== RUN ====================\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Using device: {Config.DEVICE}\")\n",
        "    print(f\"Loss type: {Config.LOSS_TYPE}\")\n",
        "    print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
        "\n",
        "    train()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
